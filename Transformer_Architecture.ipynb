{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jJBf948dzH2c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from math import sqrt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def scaled_dot_transformation(query,key,value):\n",
        "  dim_k = key.size(-1)\n",
        "  scores = torch.bmm(query,key.transpose(1,2))//sqrt(dim_k)\n",
        "  #dim=1 ensures that the softmax operation is applied independently to each row (sample) in the scores tensor.\n",
        "  weights = F.Softmax(scores,dim=1)\n",
        "\n",
        "  return torch.bmm(weights,value)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*embed_dim:* This represents the number of dimensions in the input feature space.For example, if each word in a sentence is represented by a 300-dimensional embedding vector, then embed_dim would be 300.\n",
        "\n",
        " *(head_dim)* is often a fraction of the total embedding dimension (embed_dim). Specifically, they recommend setting head_dim as embed_dim / num_heads, where num_heads is the number of attention heads.\n",
        "\n",
        "tensors of shape [batch_size, seq_len,\n",
        "head_dim]:\n",
        " For example, if you have 64 sequences (sentences) in a batch, each with 10 words, and each word represented by a 300-dimensional embedding, the input tensor shape would be (64, 10, 300)"
      ],
      "metadata": {
        "id": "b504F0XHz3P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Code for Single Attention Head\n",
        "\n",
        "class AttentionHead(nn.Module): #nn.Module = Building Block of Model as base class in Pytorch\n",
        "\n",
        "  def __init__(self,embed_dim,head_dim):\n",
        "    super().__init__():\n",
        "    self.q = nn.Linear(embed_dim,head_dim) #Like Dense Layer in TensorFlow\n",
        "    self.k = nn.Linear(embed_dim,head_dim)\n",
        "    self.v = nn.Linear(embed_dim,head_dim)\n",
        "\n",
        "  def forward(self,hidden_state):\n",
        "    attn_outputs = scaled_dot_transformation(self.q(hidden_state),\n",
        "                                             self.k(hidden_state),self.v(hidden_state))\n",
        "    return attn_outputs"
      ],
      "metadata": {
        "id": "ggN0imkb6Mqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have single atention head we can concatenate outputs of each to have multi head attention layer."
      ],
      "metadata": {
        "id": "hm1hnGSX0-S3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def__init__(self,config):\n",
        "    super()__init__()\n",
        "    embed_dim = config.hidden_size\n",
        "    num_heads = config.num_attention_head\n",
        "    head_dim = embed_dim//num_heads\n",
        "    self.heads = nn.ModuleList(\n",
        "        [AttentionHead(embed_dim,head_dim) for _ in range(num_heads)]\n",
        "    )\n",
        "    self.output_linear = nn.Linear(embed_dim,embed_dim)\n",
        "\n",
        "\n",
        "    def forward(self,hidden_state):\n",
        "      x = torch.cat([h(hidden_state) for h in self.heads],dim = -1)\n",
        "      x = self.output_linear(x)\n",
        "      return x\n",
        "\n"
      ],
      "metadata": {
        "id": "xV5zVu-X09zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In above code we making number of multi heads we need.\n",
        "- In *def forward(self,hidden)*: We concatenating the results obtained from each hidden layer along the last dim (dim=-1)\n",
        "As return the concatenated tensor for the furthur process.\n",
        "\n",
        "- Notice that the concatenated output from the attention heads is also fed through a\n",
        "final linear layer to produce an output tensor of shape [batch_size, seq_len,\n",
        "hidden_dim]"
      ],
      "metadata": {
        "id": "x5tsdIAKOXP3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H_PEGmv_OVk8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}